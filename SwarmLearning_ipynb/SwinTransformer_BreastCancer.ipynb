{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import psutil\n",
    "import time\n",
    "import statistics\n",
    "from glob import glob\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set some parameters\n",
    "NUM_NODES = 5 \n",
    "EPOCHS = 10     \n",
    "BATCH_SIZE = 32 \n",
    "DATA_PATTERN = [0.3, 0.7]  # Data distribution pattern: Node 1: 30%, the remaining nodes evenly distribute: 70%.\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = set()  # Create a set to store unique labels\n",
    "\n",
    "        # Iterate over patient ID folders\n",
    "        for patient_id in sorted(os.listdir(root)):\n",
    "            patient_path = os.path.join(root, patient_id)\n",
    "            for class_label in ['0', '1']:\n",
    "                class_path = os.path.join(patient_path, class_label)\n",
    "                if os.path.isdir(class_path):\n",
    "                    for img_name in os.listdir(class_path):\n",
    "                        img_path = os.path.join(class_path, img_name)\n",
    "                        label = int(class_label)\n",
    "                        self.samples.append((img_path, label))\n",
    "                        self.labels.add(label)  # Add label to the set\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, label = self.samples[index]\n",
    "        sample = default_loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "# Define model\n",
    "class SwinTransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SwinTransformerModel, self).__init__()\n",
    "        self.swin_transformer = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
    "\n",
    "        # Freeze all parameters of the pre-trained model\n",
    "        for param in self.swin_transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get the number of input features for the last layer\n",
    "        num_features = self.swin_transformer.head.in_features\n",
    "        self.swin_transformer.head = nn.Sequential(\n",
    "            nn.Dropout(0.5),  # Adding Dropout Layers to Reduce Overfitting\n",
    "            nn.Linear(num_features, 512),  # Top level fully connected layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "        \n",
    "        # Ensure that only the parameters of the newly added fully connected layer are updated\n",
    "        for param in self.swin_transformer.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for name, param in self.swin_transformer.named_parameters():\n",
    "            if name in ['layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias']:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Add a global average pooling layer to handle the spatial dimensions\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swin_transformer.forward_features(x)  # Extract features\n",
    "\n",
    "        # Adjust the dimension order\n",
    "        x = x.permute(0, 3, 1, 2)  # From [32, 7, 7, 768] to [32, 768, 7, 7]\n",
    "\n",
    "        # Apply global average pooling\n",
    "        x = self.global_avg_pool(x)  # From [32, 768, 7, 7] to [32, 768, 1, 1]\n",
    "\n",
    "        x = torch.flatten(x, 1)  # Flatten from [32, 768, 1, 1] to [32, 768]\n",
    "        x = self.swin_transformer.head(x)  # Apply fully connected layer\n",
    "       \n",
    "        return x\n",
    "\n",
    "def check_dataset_labels(dataset):\n",
    "    # Get all unique labels in the dataset\n",
    "    all_labels = [label for _, label in dataset]\n",
    "    unique_labels = set(all_labels)\n",
    "    print(\"Unique labels in the dataset:\", unique_labels)\n",
    "    return max(unique_labels) + 1\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Use the CustomDataset\n",
    "    dataset = CustomDataset(root='D:/USYD S3/archive/IDC_regular_ps50_idx5', transform=transform)\n",
    "    num_classes = len(dataset.labels)  # Correctly access the number of unique labels\n",
    "    print(\"Number of samples in the dataset:\", len(dataset))\n",
    "    print(\"Detected number of classes:\", num_classes)\n",
    "\n",
    "    # Use DataLoader to handle batches and shuffling\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=12\n",
    "    )\n",
    "    return data_loader, num_classes\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    accuracy = 100 * correct / total\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    return accuracy, recall, f1, precision\n",
    "\n",
    "def calculate_dynamic_threshold(epoch, max_epochs, base_threshold, max_threshold=100):\n",
    "    \"\"\"\n",
    "    Calculate a dynamic GPU usage threshold based on the current epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Current epoch number.\n",
    "        max_epochs (int): Total number of epochs planned for training.\n",
    "        base_threshold (int): Calculated median GPU usage as base threshold.\n",
    "        max_threshold (int): Maximum threshold towards the end of training.\n",
    "\n",
    "    Returns:\n",
    "        int: Calculated dynamic GPU usage threshold.\n",
    "    \"\"\"\n",
    "    progression = epoch / max_epochs\n",
    "    return base_threshold + (max_threshold - base_threshold) * progression\n",
    "\n",
    "def should_node_continue(node_id, epoch, model, val_loader, threshold):\n",
    "    \"\"\"\n",
    "    Determine whether the node continues to participate in training based on GPU usage.\n",
    "\n",
    "    Args:\n",
    "        node_id (int): Node ID.\n",
    "        epoch (int): Current epoch number.\n",
    "        model (torch.nn.Module): Model instance.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation data loader.\n",
    "        threshold (int): Dynamic GPU usage threshold.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether to continue participating in training.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_usage = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100\n",
    "    else:\n",
    "        gpu_usage = 0  # Assume no GPU usage if no GPU available\n",
    "\n",
    "    print(f'Node {node_id}, Epoch {epoch}, GPU Usage: {gpu_usage}%')\n",
    "\n",
    "    if gpu_usage > threshold:\n",
    "        print(f'Node {node_id} exiting due to high GPU usage: GPU {gpu_usage}%')\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def node_process(env, node_id, net, train_loader, val_loader, global_weights, num_classes, status, all_done):\n",
    "    model = SwinTransformerModel(num_classes=num_classes).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)  # Adjust learning rate\n",
    "\n",
    "    gpu_usages = []\n",
    "    base_threshold = None\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "\n",
    "    accuracy_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    precision_list = []\n",
    "    val_accuracy_list = []\n",
    "    val_recall_list = []\n",
    "    val_f1_list = []\n",
    "    val_precision_list = []\n",
    "\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        if epoch == 0:  # First epoch, collect GPU usages\n",
    "            if torch.cuda.is_available():\n",
    "                current_usage = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100\n",
    "                gpu_usages.append(current_usage)\n",
    "\n",
    "        # After first epoch, calculate median usage as base threshold\n",
    "        if epoch == 1:\n",
    "            base_threshold = statistics.median(gpu_usages)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        true_positive = 0\n",
    "        false_negative = 0\n",
    "        false_positive = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{EPOCHS}, Node {node_id}')\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            true_positive += ((predicted == 1) & (target == 1)).sum().item()\n",
    "            false_negative += ((predicted != 1) & (target == 1)).sum().item()\n",
    "            false_positive += ((predicted == 1) & (target != 1)).sum().item()\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "            pbar.set_postfix(loss=loss.item(), accuracy=f'{100 * correct / total:.2f}%')\n",
    "\n",
    "            # Synchronize and update model across nodes\n",
    "            params = [param.data for param in model.parameters()]\n",
    "            yield env.process(broadcast_params(env, net, params))\n",
    "            params_list, weights = yield env.process(gather_params(env, net, global_weights))\n",
    "            avg_params = fed_avg(params_list)\n",
    "            with torch.no_grad():\n",
    "                for param, avg_param in zip(model.parameters(), avg_params):\n",
    "                    param.copy_(avg_param)\n",
    "        \n",
    "        # Operations after one epoch of training\n",
    "        # Calculate and record training accuracy\n",
    "        training_accuracy = 100 * correct / total\n",
    "        accuracy_list.append(training_accuracy)\n",
    "        training_precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        # Calculate and record training recall after each epoch\n",
    "        training_recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        training_f1_score = 2 * (training_precision * training_recall) / (training_precision + training_recall) if (training_precision + training_recall) > 0 else 0\n",
    "        recall_list.append(training_recall)\n",
    "        f1_list.append(training_f1_score)\n",
    "        precision_list.append(training_precision)\n",
    "\n",
    "\n",
    "        # Perform model validation\n",
    "        val_accuracy, val_recall, val_f1, val_precision = validate(model, val_loader, device)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        val_recall_list.append(val_recall)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_precision_list.append(val_precision)\n",
    "\n",
    "        # Print epoch summary\n",
    "        scheduler.step()  # Update learning rate\n",
    "        print(f'Node {node_id}, Epoch {epoch}, Loss: {loss.item()}, Training Accuracy: {training_accuracy:.2f}%, Training Recall: {training_recall:.2f}, Training F1 Score: {training_f1_score:.2f}, Training Precision Score: {training_precision:.2f}, Val Accuracy: {val_accuracy:.2f}%, Val Recall: {val_recall:.2f}, Val F1 Score: {val_f1:.2f}, Val Precision Score: {val_precision:.2f}')\n",
    "\n",
    "        # Dynamic threshold calculation starts from the second epoch\n",
    "        if epoch > 0:\n",
    "            current_threshold = calculate_dynamic_threshold(epoch, EPOCHS, base_threshold)\n",
    "            if not should_node_continue(node_id, epoch, model, val_loader, current_threshold):\n",
    "                status[node_id] = False\n",
    "                break\n",
    "            else:\n",
    "                status[node_id] = True\n",
    "\n",
    "\n",
    "\n",
    "    # Plot training and validation\n",
    "    plt.figure(figsize=(6, 18))\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(range(len(accuracy_list)), accuracy_list, 'navy', label='Training Accuracy')\n",
    "    plt.plot(range(len(val_accuracy_list)), val_accuracy_list, 'skyblue', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Training and Validation Accuracy at Node {node_id}')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(range(len(recall_list)), recall_list, 'darkred', label='Training Recall')\n",
    "    plt.plot(range(len(val_recall_list)), val_recall_list, 'salmon', label='Validation Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title(f'Training and Validation Recall at Node {node_id}')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(range(len(f1_list)), f1_list, 'darkgreen', label='Training F1 Score')\n",
    "    plt.plot(range(len(val_f1_list)), val_f1_list, 'lightgreen', label='Validation F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'Training and Validation F1 Score at Node {node_id}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(range(len(precision_list)), precision_list, 'purple', label='Training Precision Score')\n",
    "    plt.plot(range(len(val_precision_list)), val_precision_list, 'lavender', label='Validation Precision Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision Score')\n",
    "    plt.title(f'Training and Validation Precision Score at Node {node_id}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mark the node as done only if it has completed all epochs\n",
    "    if status[node_id]:\n",
    "        all_done[node_id] = True\n",
    "        print(f'Node {node_id} marked as done.')\n",
    "    yield env.timeout(1)  # Simulate delay\n",
    "\n",
    "def broadcast_params(env, net, params):\n",
    "    def broadcast_helper(env, net, params):\n",
    "        yield net.broadcast(params)\n",
    "\n",
    "    yield env.process(broadcast_helper(env, net, params))\n",
    "\n",
    "def gather_params(env, net, weights):\n",
    "    def gather_helper(env, net):\n",
    "        received_events = yield net.gather()\n",
    "        params_list = [event.value for event in received_events if event.value is not None]\n",
    "        return params_list\n",
    "\n",
    "    params_list = yield env.process(gather_helper(env, net))\n",
    "    return params_list, weights\n",
    "\n",
    "# fedAvg to merge parameters\n",
    "def fed_avg(params_list):\n",
    "    avg_params = []\n",
    "    num_nodes = len(params_list)\n",
    "    for params in zip(*params_list):\n",
    "        avg_param = sum(params) / num_nodes\n",
    "        avg_params.append(avg_param)\n",
    "    return avg_params\n",
    "\n",
    "# compute network objects\n",
    "class P2PNetwork(object):\n",
    "    def __init__(self, env, num_nodes):\n",
    "        self.env = env\n",
    "        self.num_nodes = num_nodes  # Ensure num_nodes is correctly defined and used\n",
    "        \n",
    "        self.pipes = [simpy.Store(env) for _ in range(self.num_nodes)]\n",
    "        self.delays = [random.randint(1, 10) for _ in range(self.num_nodes)]\n",
    "        \n",
    "    def broadcast(self, value):\n",
    "        events = [self.pipes[i].put(value) for i in range(self.num_nodes)]\n",
    "        return self.env.all_of(events)\n",
    "    \n",
    "    def gather(self):\n",
    "        received = [self.pipes[i].get() for i in range(self.num_nodes)]\n",
    "        delays = [self.env.timeout(self.delays[i]) for i in range(self.num_nodes)]\n",
    "        all_events = received.copy()\n",
    "        all_events.extend(delays)\n",
    "        return self.env.all_of(all_events)\n",
    "\n",
    "# main function\n",
    "def main():\n",
    "    global device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    env = simpy.Environment()\n",
    "    net = P2PNetwork(env, NUM_NODES)\n",
    "    \n",
    "    # Initialize the list to track completion status of nodes\n",
    "    all_done = [False] * NUM_NODES\n",
    "\n",
    "    # Load data and unpack the returned tuple\n",
    "    train_loader, num_classes = load_data()\n",
    "    \n",
    "    # Distribute data to nodes\n",
    "    num_samples = len(train_loader.dataset)\n",
    "    num_train = int(0.8 * num_samples)  # 80% of the dataset for training\n",
    "    num_val = num_samples - num_train  # Remaining 20% for validation\n",
    "    train_subset, val_subset = random_split(train_loader.dataset, [num_train, num_val])\n",
    "\n",
    "    # Create data loaders for training and validation subsets\n",
    "    train_subset_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "    val_subset_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(DATA_PATTERN[0] * num_train))\n",
    "    node1_indices = indices[:split]\n",
    "    remaining_indices = indices[split:]\n",
    "    num_remaining = len(remaining_indices)\n",
    "    chunk_size = num_remaining // (NUM_NODES - 1)\n",
    "    node_indices = [node1_indices] + [remaining_indices[i:i+chunk_size] for i in range(0, num_remaining, chunk_size)]\n",
    "    node_subsets = [torch.utils.data.Subset(train_subset, idx) for idx in node_indices]\n",
    "    node_loaders = [torch.utils.data.DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True) for subset in node_subsets]\n",
    "    \n",
    "    # Compute node weight (according to data size)\n",
    "    node_weights = [len(subset) / num_train for subset in node_subsets]\n",
    "\n",
    "    # Initialize status list\n",
    "    status = [True] * NUM_NODES  # Initialize node status (all active)\n",
    "\n",
    "    # Start a node process\n",
    "    processes = [env.process(node_process(env, i, net, node_loaders[i], val_subset_loader, node_weights[i], num_classes, status, all_done)) for i in range(NUM_NODES)]\n",
    "\n",
    "    while not all(all_done):  # Check if all nodes are done\n",
    "        env.run(until=env.timeout(1))\n",
    "        #print(f'Checking completion status: {all_done}')\n",
    "        # Check and restart inactive nodes in the next round\n",
    "        for i in range(NUM_NODES):\n",
    "            if not status[i] and not all_done[i]:  # Node is not done and inactive\n",
    "                print(f'Restarting node {i} for the next round')\n",
    "                status[i] = True  # Reset the status to True before restarting\n",
    "                processes.append(env.process(node_process(env, i, net, node_loaders[i], val_subset_loader, node_weights[i], num_classes, status, all_done)))\n",
    "\n",
    "    print(\"All nodes have completed training.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
